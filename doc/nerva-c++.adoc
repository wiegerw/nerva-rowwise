= Nerva-rowwise C++ manual
:copyright: Copyright 2024 Wieger Wesselink
:author: Wieger Wesselink
:email: j.w.wesselink@tue.nl
:doctype: book
:toc: left
:toc2:
:toc-title: pass:[<h3>Contents</h3>]
:css-signature: demo
:stem: latexmath
:icons: font
:description: Documentation for the nerva-rowwise repository.
:library: nerva-rowwise
:imagesdir: images
:bibliography: bibliography.bib
Wieger Wesselink <j.w.wesselink@tue.nl>
ifdef::env-github[]
:note-caption: :information_source:
:tip-caption: :bulb:
endif::[]

++++
<style>
  .small-code .content pre {
      font-size: 0.7em;
  }
</style>
++++

== Introduction
This document describes the implementation of the {library} library.

== Installation
The following build systems are supported:

- https://cmake.org/[CMake] 3.16+
- https://www.bfgroup.xyz/b2/[B2]

Using CMake, the {library} library can be built in a standard way. The only prerequisite is that the Intel OneAPI library must be installed, and two environment variables must be set. The variable `ONEAPI_ROOT` should point to the OneAPI installation directory, and the variable `MKL_DIR` to the MKL installation directory. The dependencies for doctest, Eigen, FMT, Lyra and pybind11 are resolved automatically using `FetchContent` commands. The build and install can for example be done like this on Linux:

[source,bash]
----
mkdir build
cd build
cmake .. \
    -DCMAKE_INSTALL_PREFIX=../install \
    -DCMAKE_BUILD_TYPE=RELEASE \
    -DMKL_DIR:PATH=$ONEAPI_ROOT/mkl/latest/lib/cmake/mkl
make -j8
make install
----
TIP: Setting the `MKL_DIR` variable is not necessary if the OneAPI `setvars.sh` script is run beforehand.

NOTE: For the clang and icpx compilers one should know that there is a longstanding issue between MKL and Eigen. See
https://community.intel.com/t5/Intel-oneAPI-Math-Kernel-Library/Using-MKL-2023-0-0-20221201-with-Eigen/m-p/1456044
and https://gitlab.com/libeigen/eigen/-/issues/2586 for more details.
To resolve this issue, a hack has been applied: by defining the symbol `EIGEN_COLPIVOTINGHOUSEHOLDERQR_LAPACKE_H`,
the inclusion of the offending header file is prevented.

[NOTE]
====
For the B2 build using the icpx compiler, the environment needs to be changed like this. Alternatively, the OneAPI `setvars.sh` script can be called.
[listing]
----
LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$ONEAPI_ROOT/latest/lib
PATH=$PATH:/$ONEAPI_ROOT/latest/bin
----
====

On Windows, a command like this can be used:
[source]
----
cmake .. ^
    -G "NMake Makefiles" ^
    -DCMAKE_INSTALL_PREFIX=..\install ^
    -DCMAKE_BUILD_TYPE=Release ^
    -DMKL_DIR="%ONEAPI_ROOT%\latest\lib\cmake\mkl"
nmake
nmake install
----
The unit tests can be run using the command

[source]
----
ctest -R nerva
----

TIP: The number of threads that are used at runtime by the MKL library can be controlled using the environment variables `MKL_NUM_THREADS` and `OMP_NUM_THREADS`, see also https://www.intel.com/content/www/us/en/docs/onemkl/developer-guide-linux/2024-2/techniques-to-set-the-number-of-threads.html.

== Command line tools
There are two command line tools in the `tools` directory. The tool `mkl` can be used to do benchmarking of computing sparse and dense matrix products using the MKL library, and the tool `mlp` can be used to do training experiments with multilayer perceptrons.

=== The tool mkl
An example of running the `mkl` tool is
[.small-code]
[source,bash]
----
include::../examples/doc_mkl.sh[tag=doc]
----
This will use various algorithms to calculate the product `A = B * C` with `A` a sparse matrix and `B` and `C` dense matrices.

The output may look like this
[.small-code]
[listing]
----
--- testing A = B * C (sdd_product) ---
A = 1000x1000 sparse
B = 1000x1000 dense  layout=column-major
C = 1000x1000 dense  layout=column-major

density(A) = 0.5
 0.01147s ddd_product A=column-major, B=column-major, C=column-major
 0.00793s ddd_product A=column-major, B=column-major, C=column-major
 0.00854s ddd_product A=column-major, B=column-major, C=column-major
 0.04049s sdd_product(batchsize=5, density(A)=0.499599, B=column-major, C=column-major)
 0.01998s sdd_product(batchsize=5, density(A)=0.499599, B=column-major, C=column-major)
 0.01178s sdd_product(batchsize=5, density(A)=0.499599, B=column-major, C=column-major)
 0.01114s sdd_product(batchsize=10, density(A)=0.499599, B=column-major, C=column-major)
 0.01099s sdd_product(batchsize=10, density(A)=0.499599, B=column-major, C=column-major)
 0.00666s sdd_product(batchsize=10, density(A)=0.499599, B=column-major, C=column-major)
 0.00375s sdd_product(batchsize=100, density(A)=0.499599, B=column-major, C=column-major)
 0.00734s sdd_product(batchsize=100, density(A)=0.499599, B=column-major, C=column-major)
 0.00332s sdd_product(batchsize=100, density(A)=0.499599, B=column-major, C=column-major)
 0.20097s sdd_product_forloop_eigen(density(A)=0.499599, B=column-major, C=column-major)
 0.19891s sdd_product_forloop_eigen(density(A)=0.499599, B=column-major, C=column-major)
 0.19893s sdd_product_forloop_eigen(density(A)=0.499599, B=column-major, C=column-major)
 0.23286s sdd_product_forloop_mkl(density(A)=0.499599, B=column-major, C=column-major)
 0.23298s sdd_product_forloop_mkl(density(A)=0.499599, B=column-major, C=column-major)
 0.23281s sdd_product_forloop_mkl(density(A)=0.499599, B=column-major, C=column-major)
----
Note that the very first invocation of an MKL function can be slow.

=== The tool mlp
An example invocation of the `mlp` tool is

[.small-code]
[source,bash]
----
include::../examples/doc_mpl.sh[tag=doc]
----
This will train a CIFAR-10 model using an MLP consisting of three layers with activation functions ReLU, ReLU and no activation. Note that first the CIFAR-10 binary version needs to be downloaded from https://www.cs.toronto.edu/~kriz/cifar.html.

The output may look like this:
[.small-code]
[listing]
----
=== Nerva c++ model ===
Sparse(input_size=3072, output_size=1024, density=0.042382877, optimizer=Nesterov(0.90000), activation=ReLU())
Sparse(input_size=1024, output_size=1024, density=0.06357384, optimizer=Nesterov(0.90000), activation=ReLU())
Dense(input_size=1024, output_size=10, optimizer=Nesterov(0.90000), activation=NoActivation())
loss = SoftmaxCrossEntropyLoss()
scheduler = ConstantScheduler(lr=0.01)
layer densities: 133325/3145728 (4.238%), 66662/1048576 (6.357%), 10240/10240 (100%)

epoch   0 lr: 0.01000000  loss: 2.30284437  train accuracy: 0.07904000  test accuracy: 0.08060000 time: 0.00000000s
epoch   1 lr: 0.01000000  loss: 2.14723837  train accuracy: 0.21136000  test accuracy: 0.21320000 time: 2.74594253s
epoch   2 lr: 0.01000000  loss: 1.91454245  train accuracy: 0.29976000  test accuracy: 0.29940000 time: 2.76982510s
epoch   3 lr: 0.01000000  loss: 1.78019225  train accuracy: 0.35416000  test accuracy: 0.35820000 time: 2.69554319s
epoch   4 lr: 0.01000000  loss: 1.68071066  train accuracy: 0.39838000  test accuracy: 0.40000000 time: 2.68532307s
epoch   5 lr: 0.01000000  loss: 1.59761505  train accuracy: 0.42820000  test accuracy: 0.43060000 time: 3.02131606s
----

[[mlp_tool]]
=== The mlp command line options
The `mlp` tool has the following command line options.
Note that mathematical descriptions of the layers, activation functions, loss functions, and weight initialization functions can be found in [TODO].

==== General Options
* `-?`, `-h`, `--help`
Display help information.
* `--verbose`, `-v`
Show verbose output.
* `--debug`, `-d`
Show debug output.

==== Random Generator
* `--seed <value>`
A seed value for the random generator.

==== Layer Configuration
* `--layers <value>`
A semicolon separated list of layers. For example, `--layers=ReLU;AllReLU(0.3);Linear` is used to specify a neural network with three layers with an ReLU, AllReLU and no activation function. The following layers are supported:

|===
|Specification |Description

|`Linear`
|Linear layer without activation

|`ReLU`
|Linear layer with ReLU activation

|`Sigmoid`
|Linear layer with sigmoid activation

|`Softmax`
|Linear layer with softmax activation

|`LogSoftmax`
|Linear layer with log-softmax activation

|`HyperbolicTangent`
|Linear layer with hyperbolic tangent activation

|`AllReLU`(stem:[\alpha])
|Linear layer with All ReLU activation

|`SReLU`(stem:[\alpha])
|Linear layer with SReLU activation

|`TReLU`(stem:[\epsilon])
|Linear layer with trimmed ReLU activation

|`BatchNorm`
|Batch normalization layer
|===

* `--layer-sizes <value>`
A comma-separated list of the sizes of linear layers of the multilayer perceptron. For example, `--layer-sizes=3072,1024,512,10` specifies the sizes of three linear layers. The first one has 3072 inputs and 1024 outputs, the second one 1024 inputs and 512 outputs, and the third one has 512 inputs and 10 outputs.
* `--densities <value>`
A comma-separated list of linear layer densities. By default, all linear layers are dense (i.e. have density 1.0). If only one value is
 specified, it will be used for all linear layers.
* `--dropouts <value>`
A comma-separated list of dropout rates of linear layers. By default, all linear layers have no dropout (i.e. dropout rate 0.0).
* `--overall-density <value>`
The overall density of the linear layers. This value should be in the interval stem:[[0,1]], and it specifies the fraction of the total number of weights that is non-zero. The overall density is not distributed evenly over the layers. Instead, small layers will be assigned a higher density than large layers.

==== Training Configuration
* `--epochs <value>`
The number of epochs of the training (default: 100).
* `--batch-size <value>`
The batch size of the training.
* `--no-shuffle`
Do not shuffle the dataset during training.
* `--no-statistics`
Do not display intermediate statistics during training.
* `--optimizers <value>`
A semicolon-separated list of optimizers used for linear and batch normalization layers. The following optimizers are supported:
|===
|Specification |Description

|`GradientDescent`
|Gradient descent optimization

|`Momentum`(stem:[\mu])
|Momentum optimization with momentum parameter stem:[\mu]

|`Nesterov`(stem:[\mu])
|Nesterov optimization with momentum parameter stem:[\mu]
|===

* `--learning-rate <value>`
A semicolon-separated list of learning rate schedulers of linear and batch normalization layers. If only one learning rate scheduler is specified, it is applied to all layers. The following learning rate schedulers are supported:
|===
|Specification |Description

|`Constant(lr)`
|Constant learning rate `lr`

|`TimeBased(lr, decay)`
|Adaptive learning rate with decay

|`StepBased(lr, drop_rate, change_rate)`
|Step based learning rate where the learning rate is regularly dropped
to a lower value

|`MultistepLR(lr, milestones, gamma)`
|Step based learning rate, where `milestones` contains the epoch numbers in which the learning rate is dropped.

|`Exponential(lr, change_rate)`
|Exponentially decreasing learning rate
|===
See also https://en.wikipedia.org/wiki/Learning_rate.

* `--loss <value>`
The loss function used for training the multilayer perceptron. The following loss functions are supported:
|===
|Specification |Description

|`SquaredError`
|Squared error loss.

|`CrossEntropy`
|Cross entropy loss (N.B. prone to numerical problems!)

|`LogisticCrossEntropy`
|Logistic cross entropy loss.

|`SoftmaxCrossEntropy`
|Softmax cross entropy loss. Matches `CrossEntropy` of PyTorch. Suitable for classification experiments.

|`NegativeLogLikelihood`
|Negative log likelihood loss.
|===

* `--layer-weights <value>`
The generator that is used for initializing the weights of the linear layers. The following weight generators are supported:
|===
|Specification |Description

|`Xavier`
|Xavier weights

|`XavierNormalized`
|Normalized Xavier weights

|`He`
|Kaiming He weights

|`Uniform`
|Uniform weights

|`Zero`
|All weights are zero (N.B. This usually doesn't work)
|===

* `--load-weights <value>`
Load weights and biases from a dictionary in NumPy `.npz` format.
The weight matrices should be stored with keys `W1,W2,...` and the bias vectors with keys `b1,b2,...`.
See also
link:https://numpy.org/doc/stable/reference/generated/numpy.lib.format.html[numpy.lib.format].

* `--save-weights <value>`
Save weights and biases to a dictionary in NumPy `.npz` format.
The weight matrices are stored with keys `W1,W2,...` and the bias vectors with keys `b1,b2,...`.
See also
link:https://numpy.org/doc/stable/reference/generated/numpy.lib.format.html[numpy.lib.format].

==== Dataset Options
* `--cifar10 <value>`
Specify the location of the binary version of the
link:https://www.cs.toronto.edu/~kriz/cifar.html[CIFAR-10] dataset. This is a directory with subdirectory `cifar-10-batches-bin`.
* `--dataset <value>`
Specify a synthetic dataset to use. The following datasets are supported:
|===
|Specification |Description |Features |Classes

|`checkerboard`
|A checkerboard pattern, see also link:https://kaifishr.github.io/2021/01/14/micro-mlp.html#checkerboard[checkerboard].
|2
|2

|`mini`
|A dataset with random values.
|3
|2
|===

* `--dataset-size <value>`
The size of the synthetic dataset (default: 1000).
* `--load-dataset <value>`
Load the dataset from a file in NumPy `.npz` format. See `--load-weights` for information about the format.
* `--save-dataset <value>`
Save the dataset to a file in NumPy `.npz` format. See `--save-weights` for information about the format.
* `--normalize`
Normalize the dataset.
* `--preprocessed <value>`
A directory containing datasets named `epoch0.npz`, `epoch1.npz`, ... See `--load-weights` for information about the format. This can for example be used to store preprocessed datasets. A script link:../python/tools/create_cifar10_datasets.py[create_cifar10_datasets.py] is available for creating augmented CIFAR-10 datasets.

==== Miscellaneous options
* `--precision <value>`
The precision used for printing matrices.
* `--info`
Print detailed information about the multilayer perceptron.
* `--timer`
Print timer messages.

==== Pruning and Growing Strategies
* `--prune <strategy>`
The strategy used for pruning sparse weight matrices. The following strategies are supported:
|===
|Specification |Description

|`Magnitude(<drop_fraction>)`
|Magnitude based pruning. A fraction of the weights with the smallest absolute value is dropped.

|`SET(<drop_fraction>)`
|SET pruning (TODO).

|`Threshold(<value>)`
|Weights below the given threshold are pruned.
|===

* `--grow <strategy>`
The strategy used for growing in sparse weight matrices. The following strategies are supported:
|===
|Specification |Description

|`Random`
|Weights are added at random positions (outside the support of the sparse matrix).
|===

* `--grow-weights <value>`
The weight generation function used for growing weights. See `--layer-weights` for supported values.

==== Computation Options
* `--computation <value>`
The computation mode that is used for backpropagation. This is used for performance measurements. The following computation modes are available:
|===
|Specification |Description

|`eigen`
|All computations are done using the Eigen library. Note that by setting the flag `EIGEN_USE_MKL_ALL` Eigen will attempt to use MKL library calls.

|`mkl`
|Some computations are implemented using MKL functions.

|`blas`
|Some computations are implemented using BLAS functions.

|`sycl`
|Some computations are implemented using SYCL functions.
|===

* `--clip <value>`
A threshold value used to set small elements of weight matrices to zero.
* `--threads <value>`
The number of threads used by the MKL and OMP libraries.
* `--gradient-step <value>`
If this value is set, gradient checks are performed with the given step size. This is very slow, and should only be used for debugging.

== Overview of the code
This section gives an overview of the C++ code in the
{library} library, and some information that is needed for understanding the code.

=== Number type
The {library} library uses a type called `scalar` as its number type. By default, it is defined as a 32-bit float. It is possible to change this by defining the symbol `NERVA_USE_DOUBLE`, in which case 64 bit doubles are used. The corresponding code is
[.small-code]
[source,cpp]
----
include::../include/nerva/neural_networks/settings.h[tag=doc]
----
A more generic approach would be to add a template argument for the number type to most classes and functions. This has been tried in the past, but since it had a negative impact on the readability of the code, it was later removed.

=== Header files
The most important header files in are given in the table below.

|===
|Header file |Description

|`link:https://github.com/wiegerw/nerva-rowwise/blob/main/include/nerva/neural_networks/multilayer_perceptron.h[multilayer_perceptron.h]`
|A multilayer perceptron class.

|`link:https://github.com/wiegerw/nerva-rowwise/blob/main/include/nerva/neural_networks/layers.h[layers.h]`
|Neural network layers.

|`link:https://github.com/wiegerw/nerva-rowwise/blob/main/include/nerva/neural_networks/activation_functions.h[activation_functions.h]`
|Activation functions.

|`link:https://github.com/wiegerw/nerva-rowwise/blob/main/include/nerva/neural_networks/loss_functions.h[loss_functions.h]`
|Loss functions.

|`link:https://github.com/wiegerw/nerva-rowwise/blob/main/include/nerva/neural_networks/weights.h[weights.h]`
|Weight initialization functions.

|`link:https://github.com/wiegerw/nerva-rowwise/blob/main/include/nerva/neural_networks/optimizers.h[optimizers.h]`
|Optimizer functions, for updating neural network parameters using their gradients.

|`link:https://github.com/wiegerw/nerva-rowwise/blob/main/include/nerva/neural_networks/learning_rate_schedulers.h[learning_rate_schedulers.h]`
|Learning rate schedulers, for updating the learning rate during training.

|`link:https://github.com/wiegerw/nerva-rowwise/blob/main/include/nerva/neural_networks/training.h[training.h]`
|A stochastic gradient descent algorithm.

|`link:https://github.com/wiegerw/nerva-rowwise/blob/main/include/nerva/neural_networks/prune.h[prune.h]`
|Algorithms for pruning sparse weight matrices. This is used for dynamic sparse training.

|`link:https://github.com/wiegerw/nerva-rowwise/blob/main/include/nerva/neural_networks/grow.h[grow.h]`
|Algorithms for (re-)growing sparse weights. This is used for dynamic sparse training.

|===

==== Class multilayer_perceptron
A multilayer perceptron (MLP) is modeled using the class `multilayer_perceptron`. It contains a list of layers, and has member functions `feedforward`, `backpropagate` and `optimize` that can be used for training the neural network. Constructing an MLP can be done manually, as is illustrated in the tests:
[.small-code]
[source,cpp]
----
include::../tests/multilayer_perceptron_test.cpp[tag=doc]
----
This will create an MLP with three linear layers that have weight matrices `W1, W2, W3` and bias vectors `b1, b2, b3`. The parameter `sizes` contains the input and output sizes of the three layers. Note that the layers and the optimizers are stored using smart pointers. This is done to facilitate the Nerva Python interface. Constructing an MLP like this is quite verbose. An easier way to construct MLPs is provided by the function `make_layers`, that offers a string based interface.

[.small-code]
[source,cpp]
----
include::../tests/gradient_test.cpp[tag=construct_mlp]
----
Note that the random number generator argument is used for the generation of the weights. See
<<mlp_tool, mlp command line options>> for an overview of the supported string arguments.

==== Class neural_network_layer
The class `neural_network_layer` is the base class of all neural network layers. It has attributes for the input matrix `X` and the corresponding gradient `DX`. Usually a layer has some additional parameters that can be learned by training the neural network. The most important member functions of `neural_network_layer` are given below.
[.small-code]
[source,cpp]
----
include::../include/nerva/neural_networks/layers.h[tag=layer]
----

==== Class loss_function
The class `loss_function` is the base class of all loss functions. Although a loss function is similar to a layer, the interface is different:
[.small-code]
[source,cpp]
----
include::../include/nerva/neural_networks/loss_functions.h[tag=doc]
----
So instead of the names `feedforward` and `backpropagate`, we use `value` and `gradient`.

==== Activation functions
Currently, there is no common base class for activation functions. For example, the ReLU activation function is implemented like this:
[.small-code]
[source,cpp]
----
include::../include/nerva/neural_networks/activation_functions.h[tag=doc]
----

NOTE: Currently, there are some inconsistencies between the
interfaces of layers, loss functions and activation functions. This may be changed in the future.

// == Matrix operations
//
// == Sparse neural networks
//
// === Static sparse training
//
// === Dynamic sparse training
//
// == Performance
//
// === Intel MKL library
//
// === Subnormal numbers

== References
This is a citation to cite:[wesselink2024nervatrulysparseimplementation].

bibliography::[]